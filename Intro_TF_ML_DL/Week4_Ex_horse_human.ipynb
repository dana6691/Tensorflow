{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week4_Ex_horse_human.ipynb","provenance":[],"authorship_tag":"ABX9TyMzwU01BFo/sxirNR81ofA+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"tiEjeAwOaR4P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1596760415429,"user_tz":300,"elapsed":7443,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"bd2f931e-541c-4a5a-d993-96223835aa86"},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n","    -O /tmp/horse-or-human.zip"],"execution_count":5,"outputs":[{"output_type":"stream","text":["--2020-08-07 00:33:29--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.125.128, 74.125.23.128, 74.125.203.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.125.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 149574867 (143M) [application/zip]\n","Saving to: ‘/tmp/horse-or-human.zip’\n","\n","/tmp/horse-or-human 100%[===================>] 142.65M  32.1MB/s    in 4.4s    \n","\n","2020-08-07 00:33:33 (32.1 MB/s) - ‘/tmp/horse-or-human.zip’ saved [149574867/149574867]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xDGVyl32bdK1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1596760417221,"user_tz":300,"elapsed":9028,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"4ea1f5f3-b397-4a2d-c751-b8be492fdc69"},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n","    -O /tmp/validation-horse-or-human.zip"],"execution_count":6,"outputs":[{"output_type":"stream","text":["--2020-08-07 00:33:35--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.204.128, 108.177.97.128, 74.125.203.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.204.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11480187 (11M) [application/zip]\n","Saving to: ‘/tmp/validation-horse-or-human.zip’\n","\n","\r          /tmp/vali   0%[                    ]       0  --.-KB/s               \r/tmp/validation-hor 100%[===================>]  10.95M  --.-KB/s    in 0.1s    \n","\n","2020-08-07 00:33:35 (93.2 MB/s) - ‘/tmp/validation-horse-or-human.zip’ saved [11480187/11480187]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Et_uhUDsft7a","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596760419268,"user_tz":300,"elapsed":2037,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}}},"source":["#Zip off data\n","import os\n","import zipfile\n","\n","local_zip = '/tmp/horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp/horse-or-human')\n","local_zip = '/tmp/validation-horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp/validation-horse-or-human')\n","zip_ref.close()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"d65eo9SLbgSF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596760426886,"user_tz":300,"elapsed":2279,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"44224195-dbfe-4d79-cee5-5c3021ad3b83"},"source":["#1)Data preprocessing\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","#Directory of data\n","train_dir = os.path.join('/tmp/horse-or-human')\n","test_dir = os.path.join('/tmp/validation-horse-or-human')\n","\n","#Normalize the pixels\n","train_datagen = ImageDataGenerator(rescale= 1/255)\n","test_datagen = ImageDataGenerator(rescale= 1/255)\n","\n","#Images flow in from the training directory\n","train_generator = train_datagen.flow_from_directory(train_dir,\n","                                                        target_size = (150,150), # all images resize to 150x150\n","                                                        batch_size = 128, # how big are the batches? 32, 64, 128 are usually good numbers\n","                                                        class_mode = 'binary') # binary classification\n","\n","test_generator = test_datagen.flow_from_directory(test_dir,\n","                                                        target_size = (150,150), \n","                                                        batch_size = 128, \n","                                                        class_mode = 'binary') # binary classification"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Found 1027 images belonging to 2 classes.\n","Found 256 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GT0vhch2iUFW","colab_type":"text"},"source":["Add convolutional layers as in the previous example, and flatten the final result to feed into the densely connected layers.\n","\n","Finally we add the densely connected layers.\n","\n","*Note that we are facing a two-class classification problem, i.e. a binary classification problem, we will end our network with a `sigmoid activation` so that the output of our network will be a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0)."]},{"cell_type":"code","metadata":{"id":"7pLiWve8iGTa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":459},"executionInfo":{"status":"ok","timestamp":1595473554982,"user_tz":300,"elapsed":4538,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"e8fe8325-33d7-481b-da6e-03e7eb41340e"},"source":["#2)Build model\n","import tensorflow as tf\n","model = tf.keras.models.Sequential([#the first convolution\n","                                    tf.keras.layers.Conv2D(16,(3,3),activation = 'relu',input_shape=(150,150,3)),\n","                                    tf.keras.layers.MaxPooling2D(2,2),\n","                                    #the second convolution\n","                                    tf.keras.layers.Conv2D(32, (3,3),activation = 'relu'),\n","                                    tf.keras.layers.MaxPooling2D(2,2),\n","                                    #the third convolution\n","                                    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","                                    tf.keras.layers.MaxPooling2D(2,2),\n","                                    #Flatten into DNN\n","                                    tf.keras.layers.Flatten(),\n","                                    tf.keras.layers.Dense(512, activation='relu'),\n","                                    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","model.summary()\n","\n","#3)Compile\n","from tensorflow.keras.optimizers import RMSprop\n","model.compile(loss='binary_crossentropy', #binary classification\n","              optimizer=RMSprop(lr=0.001),\n","              metrics=['acc'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_9\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_27 (Conv2D)           (None, 148, 148, 16)      448       \n","_________________________________________________________________\n","max_pooling2d_27 (MaxPooling (None, 74, 74, 16)        0         \n","_________________________________________________________________\n","conv2d_28 (Conv2D)           (None, 72, 72, 32)        4640      \n","_________________________________________________________________\n","max_pooling2d_28 (MaxPooling (None, 36, 36, 32)        0         \n","_________________________________________________________________\n","conv2d_29 (Conv2D)           (None, 34, 34, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_29 (MaxPooling (None, 17, 17, 64)        0         \n","_________________________________________________________________\n","flatten_9 (Flatten)          (None, 18496)             0         \n","_________________________________________________________________\n","dense_18 (Dense)             (None, 512)               9470464   \n","_________________________________________________________________\n","dense_19 (Dense)             (None, 1)                 513       \n","=================================================================\n","Total params: 9,494,561\n","Trainable params: 9,494,561\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZkoJLUeWlEV-","colab_type":"text"},"source":["The \"output shape\" column shows the size of the feature map in each layer. The convolution layer reduced size of the feature maps by padding, and each pooling layer halves the dimensions.\n","\n","Next, we'll configure the specifications for model training. We will train our model with the binary_crossentropy loss, because it's a binary classification problem and our final activation is a sigmoid. We will use the rmsprop optimizer with a learning rate of 0.001. During training, we will want to monitor classification accuracy.\n","\n","NOTE: Using the RMSprop optimization algorithm is preferable to stochastic gradient descent (SGD), because RMSprop automates learning-rate tuning for us. (Other optimizers, such as Adam and Adagrad, also automatically adapt the learning rate during training, and would work equally well here.)\n","\n","*Optimizers like RMSProp, Adam and Adagrad all adaptively change the learning rate. It's important to adjust the learning rate as a model learns more, too big and the gradients could explode, too small and the gradients could vanish."]},{"cell_type":"code","metadata":{"id":"aoCRx6ZigHd7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":802},"executionInfo":{"status":"ok","timestamp":1595473935763,"user_tz":300,"elapsed":385295,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"91239d11-fcee-47f7-99fc-6abfa0d9ecac"},"source":["#4)Training the model\n","history = model.fit_generator(\n","      train_generator,\n","      steps_per_epoch=8,  \n","      epochs=15,\n","      verbose=1,\n","      validation_data = test_generator,\n","      validation_steps=8)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/15\n","8/8 [==============================] - ETA: 0s - loss: 4.0173 - acc: 0.5006WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 21s 3s/step - loss: 4.0173 - acc: 0.5006 - val_loss: 0.6415 - val_acc: 0.5234\n","Epoch 2/15\n","8/8 [==============================] - ETA: 0s - loss: 0.7870 - acc: 0.6741WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 21s 3s/step - loss: 0.7870 - acc: 0.6741 - val_loss: 0.6446 - val_acc: 0.5586\n","Epoch 3/15\n","8/8 [==============================] - ETA: 0s - loss: 0.4502 - acc: 0.8265WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 22s 3s/step - loss: 0.4502 - acc: 0.8265 - val_loss: 1.6059 - val_acc: 0.5508\n","Epoch 4/15\n","8/8 [==============================] - ETA: 0s - loss: 0.2486 - acc: 0.8799WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 22s 3s/step - loss: 0.2486 - acc: 0.8799 - val_loss: 0.7724 - val_acc: 0.8164\n","Epoch 5/15\n","8/8 [==============================] - ETA: 0s - loss: 0.1531 - acc: 0.9466WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 21s 3s/step - loss: 0.1531 - acc: 0.9466 - val_loss: 0.3931 - val_acc: 0.8125\n","Epoch 6/15\n","8/8 [==============================] - ETA: 0s - loss: 0.1975 - acc: 0.9099WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 22s 3s/step - loss: 0.1975 - acc: 0.9099 - val_loss: 3.2851 - val_acc: 0.5352\n","Epoch 7/15\n","8/8 [==============================] - ETA: 0s - loss: 0.3590 - acc: 0.8576WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 22s 3s/step - loss: 0.3590 - acc: 0.8576 - val_loss: 0.9738 - val_acc: 0.8203\n","Epoch 8/15\n","8/8 [==============================] - ETA: 0s - loss: 0.0453 - acc: 0.9855WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 24s 3s/step - loss: 0.0453 - acc: 0.9855 - val_loss: 1.5644 - val_acc: 0.7852\n","Epoch 9/15\n","8/8 [==============================] - ETA: 0s - loss: 0.0269 - acc: 0.9889WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 25s 3s/step - loss: 0.0269 - acc: 0.9889 - val_loss: 1.0496 - val_acc: 0.8555\n","Epoch 10/15\n","8/8 [==============================] - ETA: 0s - loss: 0.0412 - acc: 0.9822WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 21s 3s/step - loss: 0.0412 - acc: 0.9822 - val_loss: 0.6793 - val_acc: 0.9023\n","Epoch 11/15\n","8/8 [==============================] - ETA: 0s - loss: 0.6159 - acc: 0.8765WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 22s 3s/step - loss: 0.6159 - acc: 0.8765 - val_loss: 0.8679 - val_acc: 0.8281\n","Epoch 12/15\n","8/8 [==============================] - ETA: 0s - loss: 0.0602 - acc: 0.9778WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 21s 3s/step - loss: 0.0602 - acc: 0.9778 - val_loss: 0.8263 - val_acc: 0.8633\n","Epoch 13/15\n","8/8 [==============================] - ETA: 0s - loss: 0.0351 - acc: 0.9911WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 21s 3s/step - loss: 0.0351 - acc: 0.9911 - val_loss: 1.3211 - val_acc: 0.8359\n","Epoch 14/15\n","8/8 [==============================] - ETA: 0s - loss: 0.0102 - acc: 0.9989WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 22s 3s/step - loss: 0.0102 - acc: 0.9989 - val_loss: 1.4582 - val_acc: 0.8359\n","Epoch 15/15\n","8/8 [==============================] - ETA: 0s - loss: 0.0064 - acc: 1.0000WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 8 batches). You may need to use the repeat() function when building your dataset.\n","8/8 [==============================] - 21s 3s/step - loss: 0.0064 - acc: 1.0000 - val_loss: 1.6590 - val_acc: 0.8281\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FIjKysT8hsr5","colab_type":"text"},"source":["*fit_generator() should use instead of fit() when we are using image generators. "]},{"cell_type":"code","metadata":{"id":"whrbmnnxpesS","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":55},"executionInfo":{"status":"ok","timestamp":1595474043600,"user_tz":300,"elapsed":26983,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"fd23ebfd-b904-4f11-ea83-7a9a9d3b6140"},"source":["#5)Running Model\n","from google.colab import files\n","from keras.preprocessing import image\n","\n","uploaded = files.upload()\n","\n","for i in uploaded.keys():\n","  img = image.load_img('/content'+i, target_size=(150,150))\n","  x = image.img_to_array(img)\n","  x = np.expand_dims(x, axis=0)\n","\n","  images = np.vstack([x])\n","  classes = model.predict(images, batch_size=10)\n","  print(classes[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-508282cf-f40c-4490-bee6-ba81cf473d69\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-508282cf-f40c-4490-bee6-ba81cf473d69\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"wL1U721DWMDR","colab_type":"text"},"source":["# Exercise 4"]},{"cell_type":"code","metadata":{"id":"SqyLLYihVcii","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":632},"executionInfo":{"status":"ok","timestamp":1596760784301,"user_tz":300,"elapsed":321218,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"fec71cf8-7aac-43d1-fbad-1e13171fa0d6"},"source":["import tensorflow as tf\n","def train_happy_sad_model():\n","    # Please write your code only where you are indicated.\n","    # please do not remove # model fitting inline comments.\n","\n","    class myCallback(tf.keras.callbacks.Callback):\n","        def on_epoch_end(self, epoch, logs={}):\n","            if(logs.get('acc')>0.99):\n","                print(\"\\nReached 99% accuracy so cancelling training!\")\n","                self.model.stop_training = True\n","                    \n","\n","    callbacks = myCallback()\n","    \n","    # This Code Block should Define and Compile the Model. Please assume the images are 150 X 150 in your implementation.\n","    model = tf.keras.models.Sequential([\n","                                    tf.keras.layers.Conv2D(16,(3,3),activation = 'relu',input_shape=(150,150,3)),\n","                                    tf.keras.layers.MaxPooling2D(2,2),\n","                                    #the second convolution\n","                                    tf.keras.layers.Conv2D(32, (3,3),activation = 'relu'),\n","                                    tf.keras.layers.MaxPooling2D(2,2),\n","                                    #the third convolution\n","                                    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","                                    tf.keras.layers.MaxPooling2D(2,2),\n","                                    #Flatten into DNN\n","                                    tf.keras.layers.Flatten(),\n","                                    tf.keras.layers.Dense(512, activation='relu'),\n","                                    tf.keras.layers.Dense(1, activation='sigmoid')       \n","    ])\n","\n","    from tensorflow.keras.optimizers import RMSprop\n","\n","    model.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=0.001),metrics=['acc'])\n","        \n","\n","    # This code block should create an instance of an ImageDataGenerator called train_datagen \n","    # And a train_generator by calling train_datagen.flow_from_directory\n","\n","    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","    train_datagen = ImageDataGenerator(rescale = 1/255)\n","\n","    # Please use a target_size of 150 X 150.\n","    train_generator = train_datagen.flow_from_directory(\n","        train_dir,\n","        target_size = (150,150),\n","        batch_size = 128,\n","        class_mode = 'binary')\n","    # Expected output: 'Found 80 images belonging to 2 classes'\n","\n","    # This code block should call model.fit_generator and train for\n","    # a number of epochs.\n","    # model fitting\n","    history = model.fit_generator(\n","      train_generator,\n","      steps_per_epoch=8,  \n","      epochs=10,\n","      verbose=1)\n","    # model fitting\n","    return history.history['acc'][-1]\n","train_happy_sad_model()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Found 1027 images belonging to 2 classes.\n","WARNING:tensorflow:From <ipython-input-10-5a34344d6d81>:58: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use Model.fit, which supports generators.\n","Epoch 1/15\n","8/8 [==============================] - 18s 2s/step - loss: 1.3142 - acc: 0.5695\n","Epoch 2/15\n","8/8 [==============================] - 17s 2s/step - loss: 0.3730 - acc: 0.8409\n","Epoch 3/15\n","8/8 [==============================] - 20s 3s/step - loss: 0.2119 - acc: 0.9088\n","Epoch 4/15\n","8/8 [==============================] - 17s 2s/step - loss: 0.0660 - acc: 0.9844\n","Epoch 5/15\n","8/8 [==============================] - 17s 2s/step - loss: 0.2704 - acc: 0.8921\n","Epoch 6/15\n","8/8 [==============================] - 17s 2s/step - loss: 0.0543 - acc: 0.9811\n","Epoch 7/15\n","8/8 [==============================] - 17s 2s/step - loss: 0.0514 - acc: 0.9789\n","Epoch 8/15\n","8/8 [==============================] - 17s 2s/step - loss: 0.0416 - acc: 0.9867\n","Epoch 9/15\n","8/8 [==============================] - 17s 2s/step - loss: 0.2504 - acc: 0.9177\n","Epoch 10/15\n","8/8 [==============================] - 19s 2s/step - loss: 0.0182 - acc: 0.9961\n","Epoch 11/15\n","8/8 [==============================] - 20s 3s/step - loss: 0.0111 - acc: 0.9989\n","Epoch 12/15\n","8/8 [==============================] - 19s 2s/step - loss: 0.0052 - acc: 0.9990\n","Epoch 13/15\n","8/8 [==============================] - 17s 2s/step - loss: 0.0025 - acc: 1.0000\n","Epoch 14/15\n","8/8 [==============================] - 17s 2s/step - loss: 0.0011 - acc: 1.0000\n","Epoch 15/15\n","8/8 [==============================] - 17s 2s/step - loss: 4.9075e-04 - acc: 1.0000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{"tags":[]},"execution_count":10}]}]}