{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week3_NLP_Transfer_Learning_GloVe_Twitter_API.ipynb","provenance":[],"authorship_tag":"ABX9TyPkPn8wVOvXPyTmW8HvG20d"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"r7viXzbdk9Y8","colab_type":"text"},"source":["**Dataset**\n","\n","This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment .\n","\n","Content\n","It contains the following 6 fields:\n","\n","*   target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n","*   ids: The id of the tweet ( 2087)\n","*   date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n","*   flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n","*   user: the user that tweeted (robotickilldozr)\n","*   text: the text of the tweet (Lyx is cool)"]},{"cell_type":"code","metadata":{"id":"im7Frj4jk1Fq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"status":"ok","timestamp":1596747722590,"user_tz":300,"elapsed":9270,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"405b79c8-8a5e-4e9a-8b0f-e1a7416c0da9"},"source":["import json\n","import tensorflow as tf\n","import csv\n","import random\n","import numpy as np\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import regularizers\n","\n","embedding_dim = 100\n","max_length = 16\n","trunc_type='post'\n","padding_type='post'\n","oov_tok = \"<OOV>\"\n","training_size = 160000 #Your dataset size here. Experiment using smaller values (i.e. 16000), but don't forget to train on at least 160000 to see the best effects\n","test_portion=.1\n","\n","corpus = []\n","\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv \\\n","    -O /tmp/training_cleaned.csv"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2020-08-06 21:01:55--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.218.128, 173.194.69.128, 108.177.126.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.218.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 238942690 (228M) [application/octet-stream]\n","Saving to: ‘/tmp/training_cleaned.csv’\n","\n","/tmp/training_clean 100%[===================>] 227.87M  45.5MB/s    in 5.0s    \n","\n","2020-08-06 21:02:01 (45.5 MB/s) - ‘/tmp/training_cleaned.csv’ saved [238942690/238942690]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MgIxe1BWk8rG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"executionInfo":{"status":"ok","timestamp":1596748128545,"user_tz":300,"elapsed":5873,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"65579696-fa6f-4316-a903-d28aa8cd0573"},"source":["num_sentences = 0\n","with open(\"/tmp/training_cleaned.csv\") as csvfile:\n","  reader = csv.reader(csvfile, delimiter=',')\n","  for row in reader:\n","      list_item=['','']\n","      list_item[0] = row[5]\n","      list_item[1] = row[0]\n","      num_sentences = num_sentences + 1\n","      corpus.append(list_item)\n","\n","print(num_sentences)\n","print(len(corpus))\n","print(corpus[20])\n","corpus[:3]"],"execution_count":5,"outputs":[{"output_type":"stream","text":["1600000\n","4800000\n","[\"@alydesigns i was out most of the day so didn't get much done \", '0']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[[\"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\",\n","  '0'],\n"," [\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\",\n","  '0'],\n"," ['@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds',\n","  '0']]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"sxQn1vRek8uK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596748279575,"user_tz":300,"elapsed":11657,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}}},"source":["sentences=[]\n","labels=[]\n","random.shuffle(corpus)\n","#Label 0 or 1\n","for x in range(training_size):\n","    #print(corpus[x])\n","    sentences.append(corpus[x][0])\n","    label = 1\n","    if corpus[x][1] == '0':\n","      label = 0\n","    labels.append(label)\n","\n","tokenizer = Tokenizer() #num_words = vocab_size, oov_token=oov_tok\n","tokenizer.fit_on_texts(sentences)\n","\n","word_index = tokenizer.word_index\n","vocab_size=len(word_index)\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","\n","split = int(test_portion * training_size)\n","\n","test_sequences = np.asarray(padded[:split])\n","training_sequences = np.asarray(padded[split:training_size])\n","test_labels = np.asarray(labels[:split])\n","training_labels = np.asarray(labels[split:training_size])"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"yLZqxRWMnaz0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1596748339919,"user_tz":300,"elapsed":801,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"39815914-cc27-4548-9ba8-5a9740eb4f4b"},"source":["print(set(labels))\n","print(vocab_size)\n","print(word_index['i'])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["{0, 1}\n","134605\n","1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jthdnwlTni0y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":830},"executionInfo":{"status":"error","timestamp":1596754982875,"user_tz":300,"elapsed":6534479,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"4b40a2bc-0dda-41f7-f689-334b5a31433c"},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length),\n","    # YOUR CODE HERE - experiment with combining different types, such as convolutions and LSTMs\n","    tf.keras.layers.GlobalAveragePooling1D(),\n","    tf.keras.layers.Dense(24, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","model.summary()\n","num_epochs = 10\n","history = model.fit(training_sequences, training_labels, \n","                    epochs=num_epochs, \n","                    validation_data=(test_sequences, test_labels))\n","plot_accuracy_loss(history)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 16, 100)           13460600  \n","_________________________________________________________________\n","global_average_pooling1d (Gl (None, 100)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 24)                2424      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 25        \n","=================================================================\n","Total params: 13,463,049\n","Trainable params: 13,463,049\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","4500/4500 [==============================] - 631s 140ms/step - loss: 0.4947 - accuracy: 0.7571 - val_loss: 0.4685 - val_accuracy: 0.7749\n","Epoch 2/10\n","4500/4500 [==============================] - 634s 141ms/step - loss: 0.3795 - accuracy: 0.8282 - val_loss: 0.4836 - val_accuracy: 0.7691\n","Epoch 3/10\n","4500/4500 [==============================] - 634s 141ms/step - loss: 0.2912 - accuracy: 0.8749 - val_loss: 0.5442 - val_accuracy: 0.7588\n","Epoch 4/10\n","4500/4500 [==============================] - 635s 141ms/step - loss: 0.2246 - accuracy: 0.9072 - val_loss: 0.6112 - val_accuracy: 0.7508\n","Epoch 5/10\n","4500/4500 [==============================] - 635s 141ms/step - loss: 0.1810 - accuracy: 0.9269 - val_loss: 0.6685 - val_accuracy: 0.7425\n","Epoch 6/10\n","4500/4500 [==============================] - 630s 140ms/step - loss: 0.1520 - accuracy: 0.9392 - val_loss: 0.7352 - val_accuracy: 0.7479\n","Epoch 7/10\n","4500/4500 [==============================] - 640s 142ms/step - loss: 0.1315 - accuracy: 0.9475 - val_loss: 0.7813 - val_accuracy: 0.7457\n","Epoch 8/10\n","4500/4500 [==============================] - 721s 160ms/step - loss: 0.1148 - accuracy: 0.9552 - val_loss: 0.8417 - val_accuracy: 0.7473\n","Epoch 9/10\n","4500/4500 [==============================] - 691s 154ms/step - loss: 0.1021 - accuracy: 0.9606 - val_loss: 0.8852 - val_accuracy: 0.7385\n","Epoch 10/10\n","4500/4500 [==============================] - 680s 151ms/step - loss: 0.0919 - accuracy: 0.9641 - val_loss: 0.9572 - val_accuracy: 0.7393\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-d3ebf789e45b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplot_accuracy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'plot_accuracy_loss' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"Ct3qkPX_qaKW","colab_type":"text"},"source":["# TXT file"]},{"cell_type":"code","metadata":{"id":"wZQL80rMn8cQ","colab_type":"code","colab":{}},"source":["# Note this is the 100 dimension version of GloVe from Stanford\n","# I unzipped and hosted it on my site to make this notebook easier\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\\n","    -O /tmp/glove.6B.100d.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZWiKu06Jn_2E","colab_type":"code","colab":{}},"source":["embeddings_index = {};\n","\n","with open('/tmp/glove.6B.100d.txt') as f:\n","    for line in f:\n","        values = line.split();\n","        word = values[0];\n","        coefs = np.asarray(values[1:], dtype='float32');\n","        embeddings_index[word] = coefs;\n","\n","embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word);\n","    if embedding_vector is not None:\n","        embeddings_matrix[i] = embedding_vector;\n","\n","print(len(embeddings_matrix))\n","# Expected Output\n","# 138859\n","print(len(set(training_labels)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_U1glTGIqmjq","colab_type":"code","colab":{}},"source":["import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt\n","\n","def plot_accuracy_loss(history):\n","  #-----------------------------------------------------------\n","  # Retrieve a list of list results on training and test data\n","  # sets for each training epoch\n","  #-----------------------------------------------------------\n","  acc=history.history['accuracy']\n","  val_acc=history.history['val_accuracy']\n","  loss=history.history['loss']\n","  val_loss=history.history['val_loss']\n","\n","  epochs=range(len(acc)) # Get number of epochs\n","\n","  #------------------------------------------------\n","  # Plot training and validation accuracy per epoch\n","  #------------------------------------------------\n","  plt.plot(epochs, acc, 'b')\n","  plt.plot(epochs, val_acc, 'r')\n","  plt.title('Training and validation accuracy')\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(\"Accuracy\")\n","  plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n","\n","  plt.figure();\n","\n","  #------------------------------------------------\n","  # Plot training and validation loss per epoch\n","  #------------------------------------------------\n","  plt.plot(epochs, loss, 'b')\n","  plt.plot(epochs, val_loss, 'r')\n","  plt.title('Training and validation loss')\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(\"Loss\")\n","  plt.legend([\"Loss\", \"Validation Loss\"])\n","\n","  plt.figure();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jMMhReK7qo0o","colab_type":"text"},"source":["# Word embedding from GloVe - Transfer Learning"]},{"cell_type":"code","metadata":{"id":"t2zF1oQ3qo-C","colab_type":"code","colab":{}},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n","    tf.keras.layers.GlobalAveragePooling1D(),\n","    tf.keras.layers.Dense(24, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])"],"execution_count":null,"outputs":[]}]}