{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week1_Token.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNrRSl9zMJPreTR7jvutbgU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"i1yspusJsW8E","colab_type":"text"},"source":["# **Tokenizer Using APIs**\n","Turning text data into numbers. \n","\n","\n","\n","1.   `Tokenizer`\n","  *   num_words: maximum number of words to be tokenized\n","  *   lower=True\n","  *   oov_token = \"<OOV>\"\n","  *   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n","\n","2.   `fit_on_texts(sentences)`\n","3.   `tokenizer.word_index`\n","4.   `texts_to_sequences(sentences)`\n","5.   `pad_sequences`\n","  *  padding='post',# add the padding to the end\n","  *  truncating='post',# remove words from the end, when it's larger then maxlen\n","  *  maxlen = 50) # set the maximum length of a sequence (To make all sequences to be the same size)\n","\n","\n","Simple labelling words in your vocabulary from 1 to 10,000 (if you've got 10,000 different words).\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"S5P7VFajv0Pr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597709264067,"user_tz":300,"elapsed":2139,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"ad70d833-420a-448b-c146-8c83b795f507"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentences = [\n","    'I love my dog',\n","    'i love my cat',\n","    'He love my dog!'\n","]\n","\n","tokenizer = Tokenizer(num_words = 100)\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(word_index)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'he': 6}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nCSy3MndtpSE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597709264068,"user_tz":300,"elapsed":2111,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"05652f37-636c-41ea-d618-fd8b59a0c034"},"source":["sentences = [\n","    'I love my dog',\n","    'i love my cat',\n","    'He love my dog!'\n","]\n","type(sentences)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"szKMjz-ftsqq","colab_type":"text"},"source":["- `tokenizer.text_to_sequences()`: convert a sequence of text to a sequence of numbers.\n","    - If the word is missing from the tokenizer, it won't be encoded into the sequence. \n","\n","- Missing words in a vocabulary is handled by placing in a placeholder such as `OOV` (for **Out Of Vocabulary**) to go in place of missing words.\n","  - To use an OOV token, use the parameter `oov_token` with the `Tokenizer` class.\n","\n","- `pad_sequences` is used to make sure all sequences are the same length. That way when passed to a neural network, the matrices it accepts as input are all a uniform size."]},{"cell_type":"code","metadata":{"id":"FVkqA1erts3q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"executionInfo":{"status":"ok","timestamp":1597709264070,"user_tz":300,"elapsed":2094,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"4836c6a5-0aef-458e-c36d-4c1c65cffbaf"},"source":["#Tokening with Out of Vocabulary + padded sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","tokenizer = Tokenizer(oov_token=\"<OOV>\")\n","sentences = [\"sldkfjdlskdjf lsdkfjdlsk hello lsdkfj lsdklkfj lkslsdkfj ldksf l skdjflk\"]\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(\"Word Index =\\n\" ,word_index)\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences, padding='post') #add the padding to the end\n","print(\"\\nSequences =\", sequences)\n","print(\"\\nPadded Sequences:\\n\",padded)\n","print(\"Padded Shape:\\n\",padded.shape)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Word Index =\n"," {'<OOV>': 1, 'sldkfjdlskdjf': 2, 'lsdkfjdlsk': 3, 'hello': 4, 'lsdkfj': 5, 'lsdklkfj': 6, 'lkslsdkfj': 7, 'ldksf': 8, 'l': 9, 'skdjflk': 10}\n","\n","Sequences = [[2, 3, 4, 5, 6, 7, 8, 9, 10]]\n","\n","Padded Sequences:\n"," [[ 2  3  4  5  6  7  8  9 10]]\n","Padded Shape:\n"," (1, 9)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F7U_h5-vIVCJ","colab_type":"text"},"source":["# **BBC News Example(Stopwords)**"]},{"cell_type":"code","metadata":{"id":"7C53d3txIhI9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597709267300,"user_tz":300,"elapsed":5304,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"2f54d627-5537-452e-ac86-2d60ad5b390b"},"source":["%pip install wget"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BVPygEzVIOKU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597709267302,"user_tz":300,"elapsed":5291,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"a8a18ce9-5eb7-4c9b-cebc-b622c37ce35c"},"source":["import wget\n","wget.download('https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv')"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'bbc-text (1).csv'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"NisstGxXIybg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597709267303,"user_tz":300,"elapsed":5277,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}}},"source":["stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \n","             \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \n","             \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \n","             \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \n","             \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \n","             \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \n","             \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \n","             \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \n","             \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n","             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \n","             \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \n","             \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \n","             \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\",\n","             \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \n","             \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"iuthkBgTucnA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"status":"ok","timestamp":1597709269702,"user_tz":300,"elapsed":7648,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"287435e4-926b-4c48-fa27-e8921095bc93"},"source":["import csv\n","sentences = []\n","labels = []\n","with open(\"bbc-text.csv\", 'r') as csvfile:\n","    # Your Code here\n","    csvfile = csv.reader(csvfile, delimiter=',')\n","    next(csvfile)\n","\n","    for row in csvfile:\n","      #print(row)\n","      #break\n","      labels.append(row[0])\n","      sentence = row[1]\n","\n","      for word in stopwords:\n","        token = \" \" + word + \" \"\n","        sentence = sentence.replace(token, \" \")\n","        sentence = sentence.replace(\"  \", \" \")\n","      sentences.append(sentence)\n","\n","print(len(sentences))\n","print(sentences[0])\n","print(type(sentences))\n","#"],"execution_count":7,"outputs":[{"output_type":"stream","text":["2225\n","tv future hands viewers home theatre systems plasma high-definition tvs digital video recorders moving living room way people watch tv will radically different five years time. according expert panel gathered annual consumer electronics show las vegas discuss new technologies will impact one favourite pastimes. us leading trend programmes content will delivered viewers via home networks cable satellite telecoms companies broadband service providers front rooms portable devices. one talked-about technologies ces digital personal video recorders (dvr pvr). set-top boxes like us s tivo uk s sky+ system allow people record store play pause forward wind tv programmes want. essentially technology allows much personalised tv. also built-in high-definition tv sets big business japan us slower take off europe lack high-definition programming. not can people forward wind adverts can also forget abiding network channel schedules putting together a-la-carte entertainment. us networks cable satellite companies worried means terms advertising revenues well brand identity viewer loyalty channels. although us leads technology moment also concern raised europe particularly growing uptake services like sky+. happens today will see nine months years time uk adam hume bbc broadcast s futurologist told bbc news website. likes bbc no issues lost advertising revenue yet. pressing issue moment commercial uk broadcasters brand loyalty important everyone. will talking content brands rather network brands said tim hanlon brand communications firm starcom mediavest. reality broadband connections anybody can producer content. added: challenge now hard promote programme much choice. means said stacey jolna senior vice president tv guide tv group way people find content want watch simplified tv viewers. means networks us terms channels take leaf google s book search engine future instead scheduler help people find want watch. kind channel model might work younger ipod generation used taking control gadgets play them. might not suit everyone panel recognised. older generations comfortable familiar schedules channel brands know getting. perhaps not want much choice put hands mr hanlon suggested. end kids just diapers pushing buttons already - everything possible available said mr hanlon. ultimately consumer will tell market want. 50 000 new gadgets technologies showcased ces many enhancing tv-watching experience. high-definition tv sets everywhere many new models lcd (liquid crystal display) tvs launched dvr capability built instead external boxes. one example launched show humax s 26-inch lcd tv 80-hour tivo dvr dvd recorder. one us s biggest satellite tv companies directtv even launched branded dvr show 100-hours recording capability instant replay search function. set can pause rewind tv 90 hours. microsoft chief bill gates announced pre-show keynote speech partnership tivo called tivotogo means people can play recorded programmes windows pcs mobile devices. reflect increasing trend freeing multimedia people can watch want want.\n","<class 'list'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FHKm1EQh4ot1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1597709270710,"user_tz":300,"elapsed":8640,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"8007ab6c-c4d0-49d0-ef23-b4eaadf87698"},"source":["sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences, padding='post')\n","print(padded[0])\n","print(padded.shape)\n","tokenizer = Tokenizer(oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(len(word_index))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[1 1 1 ... 0 0 0]\n","(2225, 2442)\n","29714\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PmDXp70v49bx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1597709271203,"user_tz":300,"elapsed":9113,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"6c1fe679-7da9-425b-ba0e-7a0057e685a9"},"source":["sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences, padding='post')\n","print(padded[0])\n","print(padded.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[  96  176 1158 ...    0    0    0]\n","(2225, 2442)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eSDvVNHI5VaL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1597709344132,"user_tz":300,"elapsed":342,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"02217717-eda6-4e6b-ecb0-4b2b3812cf2a"},"source":["tokenizer = Tokenizer()\n","\n","tokenizer.fit_on_texts(labels)\n","label_word_index = tokenizer.word_index\n","\n","label_seq = tokenizer.texts_to_sequences(labels)\n","#label_seq = pad_sequences(label_seq, padding='post')\n","\n","print(label_seq)\n","print(label_word_index)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[[4], [2], [1], [1], [5], [3], [3], [1], [1], [5], [5], [2], [2], [3], [1], [2], [3], [1], [2], [4], [4], [4], [1], [1], [4], [1], [5], [4], [3], [5], [3], [4], [5], [5], [2], [3], [4], [5], [3], [2], [3], [1], [2], [1], [4], [5], [3], [3], [3], [2], [1], [3], [2], [2], [1], [3], [2], [1], [1], [2], [2], [1], [2], [1], [2], [4], [2], [5], [4], [2], [3], [2], [3], [1], [2], [4], [2], [1], [1], [2], [2], [1], [3], [2], [5], [3], [3], [2], [5], [2], [1], [1], [3], [1], [3], [1], [2], [1], [2], [5], [5], [1], [2], [3], [3], [4], [1], [5], [1], [4], [2], [5], [1], [5], [1], [5], [5], [3], [1], [1], [5], [3], [2], [4], [2], [2], [4], [1], [3], [1], [4], [5], [1], [2], [2], [4], [5], [4], [1], [2], [2], [2], [4], [1], [4], [2], [1], [5], [1], [4], [1], [4], [3], [2], [4], [5], [1], [2], [3], [2], [5], [3], [3], [5], [3], [2], [5], [3], [3], [5], [3], [1], [2], [3], [3], [2], [5], [1], [2], [2], [1], [4], [1], [4], [4], [1], [2], [1], [3], [5], [3], [2], [3], [2], [4], [3], [5], [3], [4], [2], [1], [2], [1], [4], [5], [2], [3], [3], [5], [1], [5], [3], [1], [5], [1], [1], [5], [1], [3], [3], [5], [4], [1], [3], [2], [5], [4], [1], [4], [1], [5], [3], [1], [5], [4], [2], [4], [2], [2], [4], [2], [1], [2], [1], [2], [1], [5], [2], [2], [5], [1], [1], [3], [4], [3], [3], [3], [4], [1], [4], [3], [2], [4], [5], [4], [1], [1], [2], [2], [3], [2], [4], [1], [5], [1], [3], [4], [5], [2], [1], [5], [1], [4], [3], [4], [2], [2], [3], [3], [1], [2], [4], [5], [3], [4], [2], [5], [1], [5], [1], [5], [3], [2], [1], [2], [1], [1], [5], [1], [3], [3], [2], [5], [4], [2], [1], [2], [5], [2], [2], [2], [3], [2], [3], [5], [5], [2], [1], [2], [3], [2], [4], [5], [2], [1], [1], [5], [2], [2], [3], [4], [5], [4], [3], [2], [1], [3], [2], [5], [4], [5], [4], [3], [1], [5], [2], [3], [2], [2], [3], [1], [4], [2], [2], [5], [5], [4], [1], [2], [5], [4], [4], [5], [5], [5], [3], [1], [3], [4], [2], [5], [3], [2], [5], [3], [3], [1], [1], [2], [3], [5], [2], [1], [2], [2], [1], [2], [3], [3], [3], [1], [4], [4], [2], [4], [1], [5], [2], [3], [2], [5], [2], [3], [5], [3], [2], [4], [2], [1], [1], [2], [1], [1], [5], [1], [1], [1], [4], [2], [2], [2], [3], [1], [1], [2], [4], [2], [3], [1], [3], [4], [2], [1], [5], [2], [3], [4], [2], [1], [2], [3], [2], [2], [1], [5], [4], [3], [4], [2], [1], [2], [5], [4], [4], [2], [1], [1], [5], [3], [3], [3], [1], [3], [4], [4], [5], [3], [4], [5], [2], [1], [1], [4], [2], [1], [1], [3], [1], [1], [2], [1], [5], [4], [3], [1], [3], [4], [2], [2], [2], [4], [2], [2], [1], [1], [1], [1], [2], [4], [5], [1], [1], [4], [2], [4], [5], [3], [1], [2], [3], [2], [4], [4], [3], [4], [2], [1], [2], [5], [1], [3], [5], [1], [1], [3], [4], [5], [4], [1], [3], [2], [5], [3], [2], [5], [1], [1], [4], [3], [5], [3], [5], [3], [4], [3], [5], [1], [2], [1], [5], [1], [5], [4], [2], [1], [3], [5], [3], [5], [5], [5], [3], [5], [4], [3], [4], [4], [1], [1], [4], [4], [1], [5], [5], [1], [4], [5], [1], [1], [4], [2], [3], [4], [2], [1], [5], [1], [5], [3], [4], [5], [5], [2], [5], [5], [1], [4], [4], [3], [1], [4], [1], [3], [3], [5], [4], [2], [4], [4], [4], [2], [3], [3], [1], [4], [2], [2], [5], [5], [1], [4], [2], [4], [5], [1], [4], [3], [4], [3], [2], [3], [3], [2], [1], [4], [1], [4], [3], [5], [4], [1], [5], [4], [1], [3], [5], [1], [4], [1], [1], [3], [5], [2], [3], [5], [2], [2], [4], [2], [5], [4], [1], [4], [3], [4], [3], [2], [3], [5], [1], [2], [2], [2], [5], [1], [2], [5], [5], [1], [5], [3], [3], [3], [1], [1], [1], [4], [3], [1], [3], [3], [4], [3], [1], [2], [5], [1], [2], [2], [4], [2], [5], [5], [5], [2], [5], [5], [3], [4], [2], [1], [4], [1], [1], [3], [2], [1], [4], [2], [1], [4], [1], [1], [5], [1], [2], [1], [2], [4], [3], [4], [2], [1], [1], [2], [2], [2], [2], [3], [1], [2], [4], [2], [1], [3], [2], [4], [2], [1], [2], [3], [5], [1], [2], [3], [2], [5], [2], [2], [2], [1], [3], [5], [1], [3], [1], [3], [3], [2], [2], [1], [4], [5], [1], [5], [2], [2], [2], [4], [1], [4], [3], [4], [4], [4], [1], [4], [4], [5], [5], [4], [1], [5], [4], [1], [1], [2], [5], [4], [2], [1], [2], [3], [2], [5], [4], [2], [3], [2], [4], [1], [2], [5], [2], [3], [1], [5], [3], [1], [2], [1], [3], [3], [1], [5], [5], [2], [2], [1], [4], [4], [1], [5], [4], [4], [2], [1], [5], [4], [1], [1], [2], [5], [2], [2], [2], [5], [1], [5], [4], [4], [4], [3], [4], [4], [5], [5], [1], [1], [3], [2], [5], [1], [3], [5], [4], [3], [4], [4], [2], [5], [3], [4], [3], [3], [1], [3], [3], [5], [4], [1], [3], [1], [5], [3], [2], [2], [3], [1], [1], [1], [5], [4], [4], [2], [5], [1], [3], [4], [3], [5], [4], [4], [2], [2], [1], [2], [2], [4], [3], [5], [2], [2], [2], [2], [2], [4], [1], [3], [4], [4], [2], [2], [5], [3], [5], [1], [4], [1], [5], [1], [4], [1], [2], [1], [3], [3], [5], [2], [1], [3], [3], [1], [5], [3], [2], [4], [1], [2], [2], [2], [5], [5], [4], [4], [2], [2], [5], [1], [2], [5], [4], [4], [2], [2], [1], [1], [1], [3], [3], [1], [3], [1], [2], [5], [1], [4], [5], [1], [1], [2], [2], [4], [4], [1], [5], [1], [5], [1], [5], [3], [5], [5], [4], [5], [2], [2], [3], [1], [3], [4], [2], [3], [1], [3], [1], [5], [1], [3], [1], [1], [4], [5], [1], [3], [1], [1], [2], [4], [5], [3], [4], [5], [3], [5], [3], [5], [5], [4], [5], [3], [5], [5], [4], [4], [1], [1], [5], [5], [4], [5], [3], [4], [5], [2], [4], [1], [2], [5], [5], [4], [5], [4], [2], [5], [1], [5], [2], [1], [2], [1], [3], [4], [5], [3], [2], [5], [5], [3], [2], [5], [1], [3], [1], [2], [2], [2], [2], [2], [5], [4], [1], [5], [5], [2], [1], [4], [4], [5], [1], [2], [3], [2], [3], [2], [2], [5], [3], [2], [2], [4], [3], [1], [4], [5], [3], [2], [2], [1], [5], [3], [4], [2], [2], [3], [2], [1], [5], [1], [5], [4], [3], [2], [2], [4], [2], [2], [1], [2], [4], [5], [3], [2], [3], [2], [1], [4], [2], [3], [5], [4], [2], [5], [1], [3], [3], [1], [3], [2], [4], [5], [1], [1], [4], [2], [1], [5], [4], [1], [3], [1], [2], [2], [2], [3], [5], [1], [3], [4], [2], [2], [4], [5], [5], [4], [4], [1], [1], [5], [4], [5], [1], [3], [4], [2], [1], [5], [2], [2], [5], [1], [2], [1], [4], [3], [3], [4], [5], [3], [5], [2], [2], [3], [1], [4], [1], [1], [1], [3], [2], [1], [2], [4], [1], [2], [2], [1], [3], [4], [1], [2], [4], [1], [1], [2], [2], [2], [2], [3], [5], [4], [2], [2], [1], [2], [5], [2], [5], [1], [3], [2], [2], [4], [5], [2], [2], [2], [3], [2], [3], [4], [5], [3], [5], [1], [4], [3], [2], [4], [1], [2], [2], [5], [4], [2], [2], [1], [1], [5], [1], [3], [1], [2], [1], [2], [3], [3], [2], [3], [4], [5], [1], [2], [5], [1], [3], [3], [4], [5], [2], [3], [3], [1], [4], [2], [1], [5], [1], [5], [1], [2], [1], [3], [5], [4], [2], [1], [3], [4], [1], [5], [2], [1], [5], [1], [4], [1], [4], [3], [1], [2], [5], [4], [4], [3], [4], [5], [4], [1], [2], [4], [2], [5], [1], [4], [3], [3], [3], [3], [5], [5], [5], [2], [3], [3], [1], [1], [4], [1], [3], [2], [2], [4], [1], [4], [2], [4], [3], [3], [1], [2], [3], [1], [2], [4], [2], [2], [5], [5], [1], [2], [4], [4], [3], [2], [3], [1], [5], [5], [3], [3], [2], [2], [4], [4], [1], [1], [3], [4], [1], [4], [2], [1], [2], [3], [1], [5], [2], [4], [3], [5], [4], [2], [1], [5], [4], [4], [5], [3], [4], [5], [1], [5], [1], [1], [1], [3], [4], [1], [2], [1], [1], [2], [4], [1], [2], [5], [3], [4], [1], [3], [4], [5], [3], [1], [3], [4], [2], [5], [1], [3], [2], [4], [4], [4], [3], [2], [1], [3], [5], [4], [5], [1], [4], [2], [3], [5], [4], [3], [1], [1], [2], [5], [2], [2], [3], [2], [2], [3], [4], [5], [3], [5], [5], [2], [3], [1], [3], [5], [1], [5], [3], [5], [5], [5], [2], [1], [3], [1], [5], [4], [4], [2], [3], [5], [2], [1], [2], [3], [3], [2], [1], [4], [4], [4], [2], [3], [3], [2], [1], [1], [5], [2], [1], [1], [3], [3], [3], [5], [3], [2], [4], [2], [3], [5], [5], [2], [1], [3], [5], [1], [5], [3], [3], [2], [3], [1], [5], [5], [4], [4], [4], [4], [3], [4], [2], [4], [1], [1], [5], [2], [4], [5], [2], [4], [1], [4], [5], [5], [3], [3], [1], [2], [2], [4], [5], [1], [3], [2], [4], [5], [3], [1], [5], [3], [3], [4], [1], [3], [2], [3], [5], [4], [1], [3], [5], [5], [2], [1], [4], [4], [1], [5], [4], [3], [4], [1], [3], [3], [1], [5], [1], [3], [1], [4], [5], [1], [5], [2], [2], [5], [5], [5], [4], [1], [2], [2], [3], [3], [2], [3], [5], [1], [1], [4], [3], [1], [2], [1], [2], [4], [1], [1], [2], [5], [1], [1], [4], [1], [2], [3], [2], [5], [4], [5], [3], [2], [5], [3], [5], [3], [3], [2], [1], [1], [1], [4], [4], [1], [3], [5], [4], [1], [5], [2], [5], [3], [2], [1], [4], [2], [1], [3], [2], [5], [5], [5], [3], [5], [3], [5], [1], [5], [1], [3], [3], [2], [3], [4], [1], [4], [1], [2], [3], [4], [5], [5], [3], [5], [3], [1], [1], [3], [2], [4], [1], [3], [3], [5], [1], [3], [3], [2], [4], [4], [2], [4], [1], [1], [2], [3], [2], [4], [1], [4], [3], [5], [1], [2], [1], [5], [4], [4], [1], [3], [1], [2], [1], [2], [1], [1], [5], [5], [2], [4], [4], [2], [4], [2], [2], [1], [1], [3], [1], [4], [1], [4], [1], [1], [2], [2], [4], [1], [2], [4], [4], [3], [1], [2], [5], [5], [4], [3], [1], [1], [4], [2], [4], [5], [5], [3], [3], [2], [5], [1], [5], [5], [2], [1], [3], [4], [2], [1], [5], [4], [3], [3], [1], [1], [2], [2], [2], [2], [2], [5], [2], [3], [3], [4], [4], [5], [3], [5], [2], [3], [1], [1], [2], [4], [2], [4], [1], [2], [2], [3], [1], [1], [3], [3], [5], [5], [3], [2], [3], [3], [2], [4], [3], [3], [3], [3], [3], [5], [5], [4], [3], [1], [3], [1], [4], [1], [1], [1], [5], [4], [5], [4], [1], [4], [1], [1], [5], [5], [2], [5], [5], [3], [2], [1], [4], [4], [3], [2], [1], [2], [5], [1], [3], [5], [1], [1], [2], [3], [4], [4], [2], [2], [1], [3], [5], [1], [1], [3], [5], [4], [1], [5], [2], [3], [1], [3], [4], [5], [1], [3], [2], [5], [3], [5], [3], [1], [3], [2], [2], [3], [2], [4], [1], [2], [5], [2], [1], [1], [5], [4], [3], [4], [3], [3], [1], [1], [1], [2], [4], [5], [2], [1], [2], [1], [2], [4], [2], [2], [2], [2], [1], [1], [1], [2], [2], [5], [2], [2], [2], [1], [1], [1], [4], [2], [1], [1], [1], [2], [5], [4], [4], [4], [3], [2], [2], [4], [2], [4], [1], [1], [3], [3], [3], [1], [1], [3], [3], [4], [2], [1], [1], [1], [1], [2], [1], [2], [2], [2], [2], [1], [3], [1], [4], [4], [1], [4], [2], [5], [2], [1], [2], [4], [4], [3], [5], [2], [5], [2], [4], [3], [5], [3], [5], [5], [4], [2], [4], [4], [2], [3], [1], [5], [2], [3], [5], [2], [4], [1], [4], [3], [1], [3], [2], [3], [3], [2], [2], [2], [4], [3], [2], [3], [2], [5], [3], [1], [3], [3], [1], [5], [4], [4], [2], [4], [1], [2], [2], [3], [1], [4], [4], [4], [1], [5], [1], [3], [2], [3], [3], [5], [4], [2], [4], [1], [5], [5], [1], [2], [5], [4], [4], [1], [5], [2], [3], [3], [3], [4], [4], [2], [3], [2], [3], [3], [5], [1], [4], [2], [4], [5], [4], [4], [1], [3], [1], [1], [3], [5], [5], [2], [3], [3], [1], [2], [2], [4], [2], [4], [4], [1], [2], [3], [1], [2], [2], [1], [4], [1], [4], [5], [1], [1], [5], [2], [4], [1], [1], [3], [4], [2], [3], [1], [1], [3], [5], [4], [4], [4], [2], [1], [5], [5], [4], [2], [3], [4], [1], [1], [4], [4], [3], [2], [1], [5], [5], [1], [5], [4], [4], [2], [2], [2], [1], [1], [4], [1], [2], [4], [2], [2], [1], [2], [3], [2], [2], [4], [2], [4], [3], [4], [5], [3], [4], [5], [1], [3], [5], [2], [4], [2], [4], [5], [4], [1], [2], [2], [3], [5], [3], [1]]\n","{'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YD1_1Q82N-nd","colab_type":"text"},"source":["- What is the name of the object used to tokenize sentences?\n","    - Tokenizer\n","- What is the name of the method used to tokenize a list of sentences?\n","    - `fit_on_texts(sentences)`\n","- Once you have the corpus tokenized, what's the method used to encode a list of sentences to use those tokens?\n","    - `texts_to_sequences(sentences)`\n","- When initializing the tokenizer, how do you specify a token to use for unknown (out of vocabulary) words?\n","    - `oov_token=<Token>`\n","- If you don't use a token for out of vocabulary words, what happens at encoding?\n","    - The word isn't encoded, and is skipped in the sequence.\n","- If you have a number of sequences of different lengths, how do you ensure that they are understood when fed into a neural network?\n","    - Use the `pad_sequences` object from the `tensorflow.keras.preprocessing.sequence` namespace.\n","- If you have a number of sequences of different length, and call pad_sequences on them, what's the default result?\n","    - They'll get padded to the length of the longest sequence by adding zeros to the beginning of shorter ones.\n","- When padding sequences, if you want the padding to be at the end of the sequence, how do you do it?\n","    - Pass `padding='post'` to `pad_sequences` when initializing it"]}]}