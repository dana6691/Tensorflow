{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week1_2_largedataset_augmentation.ipynb","provenance":[],"authorship_tag":"ABX9TyO1/RotBPiEMCvifzxbw0T2"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GLeN6cr5utLs","colab_type":"text"},"source":["# **LargeDataset & Plot loss/accuracy & Augmentation**"]},{"cell_type":"markdown","metadata":{"id":"BDmxIXTLmk6r","colab_type":"text"},"source":["\n","```\n","#1. Dog vs Cat image classification\n","#2. 4 convolution layers with 32,64,128 and 128 convolutions\n","#3. train for 100 epochs to graph of loss and accuracy\n","```\n","Data\n","\n","The 2,000 images used in this exercise are excerpted from the \"Dogs vs. Cats\" dataset available on Kaggle, which contains 25,000 images. Here, we use a subset of the full dataset to decrease training time for educational purposes.\n"]},{"cell_type":"markdown","metadata":{"id":"nElo3YIGx1or","colab_type":"text"},"source":["**Before Augmentation**"]},{"cell_type":"code","metadata":{"id":"Qvo635LimskU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1597065086892,"user_tz":300,"elapsed":730073,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}},"outputId":"3bcf6045-5852-41c9-f785-1953302c656a"},"source":["import os\n","import zipfile\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.optimizers import RMSprop\n","#data\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n","    -O /tmp/cats_and_dogs_filtered.zip\n","\n","#extract zip to the base directory\n","zip = zipfile.ZipFile('/tmp/cats_and_dogs_filtered.zip','r')\n","zip.extractall('/tmp')\n","zip.close()\n","\n","#directory with train & validation data\n","train_dir = os.path.join('/tmp/cats_and_dogs_filtered','train')\n","validation_dir = os.path.join('/tmp/cats_and_dogs_filtered','validation')\n","\n","\"\"\"\n","train_cats_dir = os.path.join(train_dir, 'cats')\n","train_dogs_dir = os.path.join(train_dir, 'dogs')\n","validation_cats_dir = os.path.join(validation_dir, 'cats')\n","validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n","\n","train_cat_fnames = os.listdir(train_cats_dir)\n","print(train_cat_fnames[:10])\n","\n","train_dog_fnames = os.listdir(train_dogs_dir)\n","train_dog_fnames.sort()\n","print(train_dog_fnames[:10])\n","\n","print('total training cat images:', len(os.listdir(train_cats_dir)))\n","print('total training dog images:', len(os.listdir(train_dogs_dir)))\n","print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n","print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\n","\"\"\"\n","#model\n","model = tf.keras.models.Sequential([ \n","    # First convolution extracts 32 filters that are 3x3                                \n","    # image was resized to 150x150 pixels in preprocessing with 3 color channels\n","    # Convolution is followed by max-pooling layer with a 2x2 window\n","    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","\n","    # Second convolution extracts 64 filters that are 3x3\n","    # Convolution is followed by max-pooling layer with a 2x2 window\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","\n","    # Third convolution extracts 128 filters that are 3x3\n","    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","\n","    # Fourth convolution extracts 128 filters that are 3x3\n","    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","\n","    # Flatten feature map to a 1-dim so we can add fully connected layers\n","    tf.keras.layers.Flatten(),\n","    # Create a fully connected layer with ReLU activation and 512 hidden units\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    # Create output layer with a single node and sigmoid activation\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","#compile\n","model.compile(loss='binary_crossentropy',\n","              optimizer=RMSprop(lr=0.0004),\n","              metrics=['accuracy'])\n","\n","#image rescale\n","train_datagen = ImageDataGenerator(rescale=1/255)\n","validation_datagen = ImageDataGenerator(rescale=1/255)\n","\n","#flow images in batches of 20 using train_datagen/validation_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","        train_dir, \n","        target_size=(150, 150),\n","        batch_size=20,\n","        class_mode='binary')\n","validation_generator = validation_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size=(150, 150),\n","        batch_size=20,\n","        class_mode='binary')\n","\n","#fit model\n","history = model.fit_generator(\n","      train_generator,\n","      steps_per_epoch=100,  # = len(train)/batch_size = 2000images/20\n","      epochs=100,\n","      validation_data=validation_generator,\n","      validation_steps=50,  # = len(train)/batch_size = 1000images/20\n","      verbose=2)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2020-08-10 12:32:41--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.124.128, 172.217.212.128, 172.217.214.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.124.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 68606236 (65M) [application/zip]\n","Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n","\n","/tmp/cats_and_dogs_ 100%[===================>]  65.43M  56.5MB/s    in 1.2s    \n","\n","2020-08-10 12:32:42 (56.5 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n","\n","Found 2000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n","WARNING:tensorflow:From <ipython-input-1-6b5a7e3a1760>:93: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use Model.fit, which supports generators.\n","Epoch 1/100\n","100/100 - 105s - loss: 0.6968 - accuracy: 0.5325 - val_loss: 0.7088 - val_accuracy: 0.5010\n","Epoch 2/100\n","100/100 - 104s - loss: 0.6504 - accuracy: 0.6260 - val_loss: 0.6230 - val_accuracy: 0.6560\n","Epoch 3/100\n","100/100 - 106s - loss: 0.5763 - accuracy: 0.6935 - val_loss: 0.5867 - val_accuracy: 0.7030\n","Epoch 4/100\n","100/100 - 104s - loss: 0.5251 - accuracy: 0.7460 - val_loss: 0.5647 - val_accuracy: 0.7120\n","Epoch 5/100\n","100/100 - 104s - loss: 0.4662 - accuracy: 0.7745 - val_loss: 0.5408 - val_accuracy: 0.7440\n","Epoch 6/100\n","100/100 - 104s - loss: 0.4139 - accuracy: 0.8145 - val_loss: 0.5198 - val_accuracy: 0.7410\n","Epoch 7/100\n","100/100 - 103s - loss: 0.3551 - accuracy: 0.8475 - val_loss: 0.5388 - val_accuracy: 0.7510\n","Epoch 8/100\n","100/100 - 104s - loss: 0.3087 - accuracy: 0.8680 - val_loss: 0.5521 - val_accuracy: 0.7410\n","Epoch 9/100\n","100/100 - 103s - loss: 0.2398 - accuracy: 0.9040 - val_loss: 0.5946 - val_accuracy: 0.7350\n","Epoch 10/100\n","100/100 - 104s - loss: 0.1824 - accuracy: 0.9350 - val_loss: 0.7100 - val_accuracy: 0.7360\n","Epoch 11/100\n","100/100 - 104s - loss: 0.1399 - accuracy: 0.9430 - val_loss: 0.7333 - val_accuracy: 0.7370\n","Epoch 12/100\n","100/100 - 104s - loss: 0.1088 - accuracy: 0.9590 - val_loss: 0.8622 - val_accuracy: 0.7410\n","Epoch 13/100\n","100/100 - 104s - loss: 0.0823 - accuracy: 0.9710 - val_loss: 0.9416 - val_accuracy: 0.7480\n","Epoch 14/100\n","100/100 - 104s - loss: 0.0645 - accuracy: 0.9815 - val_loss: 0.9149 - val_accuracy: 0.7470\n","Epoch 15/100\n","100/100 - 108s - loss: 0.0466 - accuracy: 0.9870 - val_loss: 1.9173 - val_accuracy: 0.6940\n","Epoch 16/100\n","100/100 - 111s - loss: 0.0469 - accuracy: 0.9820 - val_loss: 1.1443 - val_accuracy: 0.7320\n","Epoch 17/100\n","100/100 - 111s - loss: 0.0572 - accuracy: 0.9820 - val_loss: 1.3294 - val_accuracy: 0.7380\n","Epoch 18/100\n","100/100 - 110s - loss: 0.0396 - accuracy: 0.9880 - val_loss: 1.2039 - val_accuracy: 0.7280\n","Epoch 19/100\n","100/100 - 110s - loss: 0.0329 - accuracy: 0.9915 - val_loss: 1.3203 - val_accuracy: 0.7410\n","Epoch 20/100\n","100/100 - 111s - loss: 0.0350 - accuracy: 0.9880 - val_loss: 2.1543 - val_accuracy: 0.7140\n","Epoch 21/100\n","100/100 - 111s - loss: 0.0290 - accuracy: 0.9905 - val_loss: 1.5651 - val_accuracy: 0.7520\n","Epoch 22/100\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6b5a7e3a1760>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# = len(train)/batch_size = 1000images/20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m       verbose=2)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m   @deprecation.deprecated(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"Z7KNaDoNMckS","colab_type":"text"},"source":[" The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the feature map."]},{"cell_type":"code","metadata":{"id":"4QJ05lkdKf2H","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1597065086897,"user_tz":300,"elapsed":46,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}}},"source":["import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","# Parameters for our graph; we'll output images in a 4x4 configuration\n","nrows = 4\n","ncols = 4\n","\n","# Index for iterating over images\n","pic_index = 0\n","\n","\n","# Set up matplotlib fig, and size it to fit 4x4 pics\n","fig = plt.gcf()\n","fig.set_size_inches(ncols * 4, nrows * 4)\n","\n","pic_index += 8\n","next_cat_pix = [os.path.join(train_cats_dir, fname) \n","                for fname in train_cat_fnames[pic_index-8:pic_index]]\n","next_dog_pix = [os.path.join(train_dogs_dir, fname) \n","                for fname in train_dog_fnames[pic_index-8:pic_index]]\n","\n","for i, img_path in enumerate(next_cat_pix+next_dog_pix):\n","  # Set up subplot; subplot indices start at 1\n","  sp = plt.subplot(nrows, ncols, i + 1)\n","  sp.axis('Off') # Don't show axes (or gridlines)\n","\n","  img = mpimg.imread(img_path)\n","  plt.imshow(img)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7RWnmqt7qfEj","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1597065086898,"user_tz":300,"elapsed":38,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}}},"source":["#Plot of accuracy and loss\n","import matplotlib.pyplot as plt\n","\n","acc = history.history[\"accuracy\"]\n","val_acc = history.history[\"val_accuracy\"]\n","loss = history.history[\"loss\"]\n","val_loss = history.history[\"val_loss\"]\n","\n","epochs = range(len(acc))\n","\n","# Plot the training and validation accuracy per epoch\n","plt.plot(epochs, acc)\n","plt.plot(epochs, val_acc)\n","plt.title(\"Training and validation accuracy\")\n","plt.figure()\n","\n","# Plot the training and validation loss per epoch\n","plt.plot(epochs, loss)\n","plt.plot(epochs, val_loss)\n","plt.title(\"Training and validation loss\")\n","plt.figure()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wLFgDUsmvnUR","colab_type":"text"},"source":["Training accuracy is over 90% and the validation accuracy is near 75%. It is a clear evidence of overfitting. "]},{"cell_type":"markdown","metadata":{"id":"Sm7O0UWmrsOd","colab_type":"text"},"source":["- What does `flow_from_directory()` give you?\n","    - The ability to easily load images, to be able to pick the size of images, to automatically label images based on their directory name.\n","- If my image is sized 150x150 and I pass a 3x3 convolution over it, what size is the resulting image?\n","    - 148x148 -> 150-3+1 = 148\n","- If my data is is sized 75x75 and I use Pooling of size 2x2, what size will the resulting image be?\n","    - 75x75 (halved)\n","- If you want to view the history of training, how can you access it?\n","    - Create a `history` variable and assign it to the return of `model.fit()`\n","- What's the name of the API that allows you to inspect the impact of convolutions on the images?\n","    - The model.layers API\n","- When checking the result graphs, the loss levelled out at about .75 after 2 epochs, but the accuracy climbed close to 1.0 after 15 epochs, what's the significance of this?\n","    - There was no point training after 2 epochs, as we overfit the training data (you want the loss to keep going down).\n","- Why is the validation accuracy a better indicator of model performance than training accuracy?\n","    - The validation accuracy is based on images that the model hasn't been trained with, and thus a better indicator of how the model will perform with new images.\n","- Why is overfitting more likely to occur on smaller datasets?\n","    - Because there's less likelihood of all possible features being encountered in the training process."]},{"cell_type":"code","metadata":{"id":"xKZkIDNev84I","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1597065086900,"user_tz":300,"elapsed":34,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}}},"source":["# Updated to do image augmentation\n","train_datagen = ImageDataGenerator(\n","      rotation_range=40,\n","      width_shift_range=0.2,\n","      height_shift_range=0.2,\n","      shear_range=0.2,\n","      zoom_range=0.2,\n","      horizontal_flip=True,\n","      fill_mode='nearest')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IvGm86xiwBy9","colab_type":"text"},"source":["* `rotation_range` is a value in degrees (0–180), a range within which to randomly rotate pictures.\n","* `width_shift` and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\n","* `shear_range` is for randomly applying shearing transformations.\n","* `zoom_range` is for randomly zooming inside pictures.\n","* `horizontal_flip` is for randomly flipping half of the images horizontally. This is relevant when there are no assumptions of horizontal assymmetry (e.g. real-world pictures).\n","* `fill_mode` is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift."]},{"cell_type":"markdown","metadata":{"id":"8Mr6PaaLxs1a","colab_type":"text"},"source":["# **Data Augmentation**\n","Manipulating an image to increase the size of your datset and help reduce the overfitting of the model.\n","\n","\n","1.   Flipping\n","2.   Rotating\n","3.   Cropping\n","4.   Skewing\n","\n","\n","\n","*    Overfitting: one of the central problem in machine learning. It happens when training set are not generalize well to unseen data."]},{"cell_type":"markdown","metadata":{"id":"axmzSxwIzMuS","colab_type":"text"},"source":["**After Aumentation Applied**"]},{"cell_type":"code","metadata":{"id":"muEgwOA0w9vK","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1597065086901,"user_tz":300,"elapsed":32,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}}},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n","    -O /tmp/cats_and_dogs_filtered.zip\n","\n","zip = zipfile.ZipFile('/tmp/cats_and_dogs_filtered.zip','r')\n","zip.extractall('/tmp')\n","zip.close()\n","\n","#directory with train & validation data\n","train_dir = os.path.join('/tmp/cats_and_dogs_filtered','train')\n","validation_dir = os.path.join('/tmp/cats_and_dogs_filtered','validation')\n","\n","#model\n","model = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","#compile\n","model.compile(loss='binary_crossentropy',\n","              optimizer=RMSprop(lr=0.0004),\n","              metrics=['accuracy'])\n","################################################################################\n","#image rescale\n","train_datagen = ImageDataGenerator(\n","      rescale=1./255,\n","      rotation_range=40,\n","      width_shift_range=0.2,\n","      height_shift_range=0.2,\n","      shear_range=0.2,\n","      zoom_range=0.2,\n","      horizontal_flip=True,\n","      fill_mode='nearest')\n","validation_datagen = ImageDataGenerator(rescale=1/255)\n","\n","#flow images in batches of 20 using train_datagen/validation_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","        train_dir, \n","        target_size=(150, 150),\n","        batch_size=20,\n","        class_mode='binary')\n","validation_generator = validation_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size=(150, 150),\n","        batch_size=20,\n","        class_mode='binary')\n","###########################################################################\n","#fit model\n","history = model.fit_generator(\n","      train_generator,\n","      steps_per_epoch=100,  # = len(train)/batch_size = 2000images/20\n","      epochs=100,\n","      validation_data=validation_generator,\n","      validation_steps=50,  # = len(train)/batch_size = 1000images/20\n","      verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ou0r1QxgxrTV","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1597065086902,"user_tz":300,"elapsed":25,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}}},"source":["#Plot of accuracy and loss\n","import matplotlib.pyplot as plt\n","\n","acc = history.history[\"accuracy\"]\n","val_acc = history.history[\"val_accuracy\"]\n","loss = history.history[\"loss\"]\n","val_loss = history.history[\"val_loss\"]\n","\n","epochs = range(len(acc))\n","\n","# Plot the training and validation accuracy per epoch\n","plt.plot(epochs, acc)\n","plt.plot(epochs, val_acc)\n","plt.title(\"Training and validation accuracy\")\n","plt.figure()\n","\n","# Plot the training and validation loss per epoch\n","plt.plot(epochs, loss)\n","plt.plot(epochs, val_loss)\n","plt.title(\"Training and validation loss\")\n","plt.figure()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F41sTyxGNoG7","colab_type":"text"},"source":["**Visualizing Intermediate Representations**\n","\n","visualize how an input gets transformed as it goes through the convnet."]},{"cell_type":"code","metadata":{"id":"ZrJrroRQNnsv","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1597065086903,"user_tz":300,"elapsed":17,"user":{"displayName":"nana","photoUrl":"","userId":"02662419698026635195"}}},"source":["import numpy as np\n","import random\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","\n","# Let's define a new Model that will take an image as input, and will output\n","# intermediate representations for all layers in the previous model after\n","# the first.\n","successive_outputs = [layer.output for layer in model.layers[1:]]\n","visualization_model = Model(img_input, successive_outputs)\n","\n","# Let's prepare a random input image of a cat or dog from the training set.\n","cat_img_files = [os.path.join(train_cats_dir, f) for f in train_cat_fnames]\n","dog_img_files = [os.path.join(train_dogs_dir, f) for f in train_dog_fnames]\n","img_path = random.choice(cat_img_files + dog_img_files)\n","\n","img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n","x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n","x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n","\n","# Rescale by 1/255\n","x /= 255\n","\n","# Let's run our image through our network, thus obtaining all\n","# intermediate representations for this image.\n","successive_feature_maps = visualization_model.predict(x)\n","\n","# These are the names of the layers, so can have them as part of our plot\n","layer_names = [layer.name for layer in model.layers]\n","\n","# Now let's display our representations\n","for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n","  if len(feature_map.shape) == 4:\n","    # Just do this for the conv / maxpool layers, not the fully-connected layers\n","    n_features = feature_map.shape[-1]  # number of features in feature map\n","    # The feature map has shape (1, size, size, n_features)\n","    size = feature_map.shape[1]\n","    # We will tile our images in this matrix\n","    display_grid = np.zeros((size, size * n_features))\n","    for i in range(n_features):\n","      # Postprocess the feature to make it visually palatable\n","      x = feature_map[0, :, :, i]\n","      x -= x.mean()\n","      x /= x.std()\n","      x *= 64\n","      x += 128\n","      x = np.clip(x, 0, 255).astype('uint8')\n","      # We'll tile each filter into this big horizontal grid\n","      display_grid[:, i * size : (i + 1) * size] = x\n","    # Display the grid\n","    scale = 20. / n_features\n","    plt.figure(figsize=(scale * n_features, scale))\n","    plt.title(layer_name)\n","    plt.grid(False)\n","    plt.imshow(display_grid, aspect='auto', cmap='viridis')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5k9MQ0aQOAhY","colab_type":"text"},"source":["As you can see, we are overfitting like it's getting out of fashion. Our training accuracy (in blue) gets close to 100% (!) while our validation accuracy (in green) stalls as 70%. Our validation loss reaches its minimum after only five epochs.\n","\n","Since we have a relatively small number of training examples (2000), overfitting should be our number one concern. Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions. For instance, if you, as a human, only see three images of people who are lumberjacks, and three images of people who are sailors, and among them the only person wearing a cap is a lumberjack, you might start thinking that wearing a cap is a sign of being a lumberjack as opposed to a sailor. You would then make a pretty lousy lumberjack/sailor classifier.\n","\n","Overfitting is the central problem in machine learning: given that we are fitting the parameters of our model to a given dataset, how can we make sure that the representations learned by the model will be applicable to data never seen before? How do we avoid learning things that are specific to the training data?\n","\n","In the next exercise, we'll look at ways to prevent overfitting in the cat vs. dog classification model."]},{"cell_type":"markdown","metadata":{"id":"HsNdpyEW7xk8","colab_type":"text"},"source":["If overfitting continues, add dropout for further regularization"]},{"cell_type":"markdown","metadata":{"id":"cLLmyWHkyhEg","colab_type":"text"},"source":["- How do you use Image Augmentation in TensorFlow?\n","    - By passing parameters to the `ImageDataGenerator`.\n","- If your training data only has people facing left, but you want to classify people facing right, how could you prevent overfitting?\n","    - Use the `horizontal_flip` parameter passed to `ImageDataGenerator`\n","- Why is training with augmentation slower?\n","    - Because the image processing (flipping, shifting, cropping images) takes cycles to complete.\n","- What does the `fill_mode` parameter of `ImageDataGenerator` do?\n","    - It attempts to recreate lost pixels after a transformation like a shear.\n","- When using Image Augmentation with the `ImageDataGenerator`, what happens to your raw image data on disk?\n","    - Nothing, all augmentation is done in-memory.\n","- How does Image Augmentation help solve overfitting?\n","    - It manipulates the training set to generate more scenarios for features in the images. Effectively increasing the size of your training set.\n","- When using Image Augmentation my training gets...\n","    - Slower. But is more robust to overfitting.\n","- Using Image Augmentation effectively simulates having a larger data set for training. True or False?\n","    - True."]}]}